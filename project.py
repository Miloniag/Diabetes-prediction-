# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-p2EHINF7jY7W7cW0R-39_nK-BXln1C
"""

import pandas as pd

df = pd.read_excel('Project.xlsx')
display(df.head())

print(df.shape)
print(df.dtypes)
print(df.describe())

for column in df.columns:
  if df[column].dtype == 'object':
    print(f"{column}: {df[column].unique()}")

# drop the 0th row and consider 1st row as labels

df.columns = df.iloc[0]
df = df[1:].reset_index(drop=True)
display(df.head())
df.shape

import pandas as pd
import matplotlib.pyplot as plt
numerical_columns = ['Age','Blood Glucose Level(BGL)','Diastolic Blood Pressure','Systolic Blood Pressure','Heart Rate','Body Temperature','SPO2']

# Convert the columns to numeric, coercing errors to NaN
for col in numerical_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')


# Drop rows where the specified columns have NaN values
df.dropna(subset=numerical_columns, inplace=True)

# Create the histogram for each numerical column
for numerical_column in numerical_columns:
    plt.figure(figsize=(10, 6))
    plt.hist(df[numerical_column], bins=20, edgecolor='black') # You can adjust the number of bins
    plt.title(f'Histogram of {numerical_column}')
    plt.xlabel(numerical_column)
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

# apply boxplot to all columns except last 3

import pandas as pd
import matplotlib.pyplot as plt
# Iterate through columns and create boxplots for relevant data types
for col in df.iloc[:, :-3].columns:
    # Check if the column contains numerical data
    if pd.api.types.is_numeric_dtype(df[col]):
        plt.figure(figsize=(10, 6))
        df.boxplot(column=col)
        plt.title(f'Boxplot of {col}')
        plt.ylabel('Values')
        plt.show()
    else:
        print(f"Skipping boxplot for non-numeric column: {col}")

# correlation matrix and heatmap of all variables

import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Create a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Variables')
plt.show()

from sklearn.model_selection import train_test_split
# Separate features (X) and target (y)
# Clean up column names by stripping leading/trailing whitespace
df.columns = df.columns.str.strip()

X = df.drop('Diabetic/Non-daibetic(BINARY)', axis=1)
y = df['Diabetic/Non-daibetic(BINARY)']

# Convert target to numeric if it's not already
# It assumes 'Diabetic/Non-daibetic(BINARY)' has two unique values representing the classes
y = y.astype('category').cat.codes

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

#apply and evaluate logistic regression

import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler

# Scale the numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply Logistic Regression
model = LogisticRegression(solver='liblinear') # 'liblinear' is good for small datasets

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] # Probability of the positive class

# Evaluate the model

# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Plot Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Diabetic', 'Diabetic'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Logistic Regression Confusion Matrix')
plt.show()

# ROC AUC Score
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"\nROC AUC Score: {roc_auc:.2f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve')
plt.legend(loc="lower right")
plt.show()

# You can also print model coefficients (optional)
print("\nModel Coefficients:")
for feature, coef in zip(X.columns, model.coef_[0]):
    print(f"{feature}: {coef:.4f}")

print("\nModel Intercept:")
model.intercept_[0]

#apply and evaluate random forest

import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

# Apply Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # You can adjust n_estimators

# Train the model
rf_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test_scaled)
y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1] # Probability of the positive class

# Evaluate the model

# Classification Report
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
print("\nRandom Forest Confusion Matrix:")
print(cm_rf)

# Plot Confusion Matrix
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Non-Diabetic', 'Diabetic'])
disp_rf.plot(cmap=plt.cm.Blues)
plt.title('Random Forest Confusion Matrix')
plt.show()

# ROC AUC Score
roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)
print(f"\nRandom Forest ROC AUC Score: {roc_auc_rf:.2f}")

# ROC Curve
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_proba_rf)
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'Random Forest ROC curve (area = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

#apply and evaluate svm

import matplotlib.pyplot as plt
from sklearn.svm import SVC

# Apply Support Vector Machine (SVM)
# Using a linear kernel as a starting point
svm_model = SVC(kernel='linear', probability=True, random_state=42)

# Train the model
svm_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_svm = svm_model.predict(X_test_scaled)
y_pred_proba_svm = svm_model.predict_proba(X_test_scaled)[:, 1] # Probability of the positive class

# Evaluate the model

# Classification Report
print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("\nSVM Confusion Matrix:")
print(cm_svm)

# Plot Confusion Matrix
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=['Non-Diabetic', 'Diabetic'])
disp_svm.plot(cmap=plt.cm.Blues)
plt.title('SVM Confusion Matrix')
plt.show()

# ROC AUC Score
roc_auc_svm = roc_auc_score(y_test, y_pred_proba_svm)
print(f"\nSVM ROC AUC Score: {roc_auc_svm:.2f}")

# ROC Curve
fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_pred_proba_svm)
plt.figure(figsize=(8, 6))
plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label=f'SVM ROC curve (area = {roc_auc_svm:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVM Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()